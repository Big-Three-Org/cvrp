Solving the Family-Capacitated VRP: Algorithmic Approaches and Implementation Plan

Overview of the F-CVRP Challenges

The Family Capacitated Vehicle Routing Problem (F-CVRP) extends the classic CVRP by grouping customers into families and requiring a fixed number of customers to be served from each family ￼. We have a fleet of vehicles with limited capacity, and we must choose exactly a certain number of customers from each family and design routes to minimize total distance. This makes F-CVRP highly complex (NP-hard) because it combines a selection problem (choosing which family members to serve) with a routing problem (ordering visits under capacity constraints). Exact methods struggle on realistic sizes ￼, so we turn to heuristic and metaheuristic algorithms (i.e. open-source solution techniques) for high-quality solutions within reasonable time. Below, we compare a range of algorithms in terms of their applicability to F-CVRP, strengths/weaknesses, and how to adapt each to the family-selection and capacity constraints.

Comparison of Algorithms for F-CVRP

Local Search (LS)

Applicability: Local search is a core component for improving any initial F-CVRP solution. It iteratively refines a single solution by making small neighbor changes (e.g. swapping two customers between routes, reordering a route segment) to reduce cost. Local search by itself will quickly produce a feasible solution that respects vehicle capacity and family selection constraints if it starts from a valid solution, but it will only find a local optimum (no small move improves cost).

Strengths: It is simple and fast to implement. LS can rapidly improve solution quality in early iterations. Common VRP local moves like 2-opt (reversing a route segment), 3-opt, swap, or relocate are well-studied and effective ￼. These moves can be extended to family selection as well – for example, a move could swap one served customer with an unserved member of the same family if it leads to a shorter route without breaking capacity. LS is highly problem-specific and can exploit the VRP structure for quick gains ￼.

Weaknesses: Pure local search often gets stuck in local optima. Once no small tweak yields improvement, LS terminates, which may be far from the global optimum. It has no built-in mechanism to escape local minima or to systematically explore drastically different selections of family members. Thus, LS alone may miss better solutions that require more significant changes (like swapping out a far-away family customer for a closer one in another route).

Adaptation to F-CVRP: We incorporate custom neighborhood moves that address family-based selection in addition to routing:
	•	Intra-route optimizations: Apply 2-opt or 3-opt within each vehicle’s route to shorten travel distance (this inherently respects capacity since it doesn’t change the set of customers, just reorders them).
	•	Inter-route moves: Swap two customers between routes or relocate a customer from one route to another to balance loads and reduce distance. In F-CVRP, these moves must ensure that each family still has the required number of members served – e.g. only swap if the two customers belong to different families (so the family counts remain correct), or if they’re from the same family the swap doesn’t change the count.
	•	Family selection moves: If a family has multiple possible customers, define a move that exchanges one selected family member for an unselected one from the same family. This changes which customers are served (affecting route composition) while keeping the family’s served count constant. After such a swap, we can locally re-optimize the routes involving those customers.

By iterating these moves (for example, repeatedly performing the best swap or 2-opt until no improvement), LS can improve an initial F-CVRP solution substantially. However, due to its tendency to stop at a local optimum, we often embed LS inside higher-level frameworks (ILS, Tabu, etc.) that provide escape mechanisms. In summary, local search is highly applicable as the workhorse for refining solutions, given its simplicity and the fact that a local optimum under one move type might not be optimum under another ￼ (so combining many move types increases solution quality).

“Promises” Strategy (Focusing on Promising Solutions)

Applicability: Although not a specific algorithm, the idea of focusing on promising solution components (sometimes called a “promising paths” or granular approach) can guide any search method. In F-CVRP, this means intelligently restricting the search to plausible choices – for example, favoring closer customer assignments and likely route structures – rather than exploring every possibility blindly. This strategy can dramatically reduce the search space while retaining high-quality solutions ￼.

Strengths: By concentrating on promising elements, the search becomes more efficient. A known example is the granular search idea in VRP, where only “good” (short distance) connections are considered for route moves. Toth and Vigo’s Granular Tabu Search, for instance, uses a restricted neighborhood that excludes moves involving very long-distance edges that are unlikely to appear in an optimal route ￼. This speeds up convergence by ignoring low-probability improvements, focusing computation on the most promising swaps or inserts. Similarly, one can apply a “promising subset” idea to family selection: for each family, identify the members that are most promising (e.g. those closest to the depot or to other customers) and prioritize those in the solution. This doesn’t mean we never consider others, but we bias the search toward likely good choices first.

Weaknesses: Over-restricting the search can inadvertently filter out the global optimum if done too aggressively. We must ensure that our criteria for “promising” are well-chosen. For example, always picking the nearest customers to the depot for each family might miss a case where a slightly farther customer enables a much more efficient route synergy. So this strategy must be used with caution and often in combination with broader search steps to ensure diversity.

Adaptation to F-CVRP: We can implement a promising-first approach in multiple ways:
	•	Initial solution bias: When constructing the initial solution, choose the required customers from each family based on a heuristic (like shortest distance to depot or forming tight clusters of customers) to start with a good foundation.
	•	Neighborhood restriction: In local search or Tabu Search, limit certain moves. For instance, only consider moving a customer to a different route if the new route is significantly closer (distance-saving) – effectively pruning moves that add a lot of distance. Only consider swapping a served family member with an unserved one if the unserved candidate has a markedly lower distance in the route context.
	•	Cost penalization: Encourage promising structures by modifying the cost function. E.g., promise to keep certain good decisions: if a particular sub-route or cluster of customers is working well, we might temporarily give it a reduced cost (or conversely, penalize breaking it) so that search algorithms tend to preserve it.

By integrating these ideas, any algorithm (LS, Tabu, GA, etc.) can be steered toward high-quality regions faster. Essentially, the “promises” strategy is about using problem insight (like distance-based intuition) to guide the search, which is an open-source heuristic principle widely used in VRP solvers to improve efficiency without sacrificing solution quality ￼ ￼.

Tabu Search (TS)

Applicability: Tabu Search is a single-solution metaheuristic that is very well-suited for F-CVRP, as it has been historically successful on VRPs of many kinds ￼ ￼. TS augments local search with a short-term memory of recent moves (the tabu list) to escape local optima. It systematically explores neighbor solutions, even allowing some worse moves, while preventing immediate backtracking. This approach can handle the complex constraints of F-CVRP by carefully designing moves and tabu conditions (for both routing and family selection changes).

Strengths: Tabu Search is known for finding high-quality solutions for large combinatorial problems. It was originally proposed to guide searches out of local optima and into promising regions ￼. The tabu memory avoids cycling back to recently visited solutions, enabling the algorithm to climb out of local minima and continue improving ￼. TS has been successfully applied to standard CVRPs and many variants – early VRP implementations in the 1990s achieved excellent results ￼. It’s also a very flexible framework: one can incorporate adaptive memory schemes and strategic diversification within TS. For example, research shows that using an adaptive memory (dynamic, multiple memory structures) in tabu can be more efficient than a fixed one ￼. Additionally, the granular tabu concept (limiting moves to likely good ones) has been integrated to speed it up without hurting solution quality ￼. Overall, TS tends to produce solutions of excellent quality, comparable to or better than other metaheuristics, especially when finely tuned. Notably, tabu search was one of the most successful heuristics for the Quadratic Assignment Problem as well – a notoriously difficult assignment-style problem – with a robust TS by Taillard (1991) outperforming many methods ￼. This indicates the power of TS to handle complex assignment-like subproblems, which is directly relevant to managing family-customer selections in F-CVRP.

Weaknesses: The main drawbacks are the complexity of implementation and parameter tuning. A basic TS requires setting a tabu tenure (how long a move or its reverse is forbidden) and possibly using aspiration criteria (allowing tabu moves if they yield a new best). These parameters greatly affect performance and may need experimentation. TS also examines a large neighborhood of moves each iteration – if not pruned, this can be slow. Efficient data structures are needed to evaluate move costs quickly (especially for VRP, where naive cost recomputation for each possible swap/insert is expensive). Moreover, TS can still get stuck in a suboptimal region if the tabu list is too short or diversification is insufficient, though long-term memory (e.g. periodically intensifying around best solutions or diversifying when improvement stalls) can alleviate this. In summary, TS is powerful but demanding to implement, and the trade-off between intensification and diversification must be managed.

Adaptation to F-CVRP: Adapting TS to F-CVRP involves extending its move repertoire:
	•	Move types: We include all the typical VRP moves (2-opt, swap between routes, relocate customer to another route) and also family exchange moves (swap a selected vs. unselected family member as described under LS). Each move must keep the solution feasible (capacities and family counts). If a move temporarily violates capacity or family count, we consider either disallowing it or treating it via a repair step. Tabu conditions can be defined on moves (e.g. recently moved customer cannot be moved back to its old route for X iterations, or a just-exchanged family member pair cannot be swapped again immediately).
	•	State memory: We maintain tabu lists for both routing decisions and selection decisions. For example, if we removed customer A of family X and replaced with B (also of X), mark that reversing this (B out, A back in) is tabu for a number of iterations to give the new choice a chance to be explored. Similarly, if we relocate a customer to a new route, forbid moving it back to the old route right away ￼.
	•	Intensification & diversification: We can use an adaptive memory pool of the best solutions found (see Adaptive Memory section) to occasionally restart TS from a different region (diversification), or focus on a best-known solution with reduced tabu tenure (intensification). Another tactic: incorporate aspiration criteria where a tabu move that yields a better solution than any seen so far is allowed – this ensures TS doesn’t miss a great move just because it’s tabu.

Overall, Tabu Search is a top contender for solving F-CVRP. It can handle the additional family selection dimension by treating it similarly to routing assignments and using its memory-based strategy to avoid getting trapped. In practice, TS often finds very good solutions for VRPs within reasonable iteration counts, making it suitable for a 5-minute runtime with careful coding (especially if we use granular neighbor pruning to speed each iteration ￼). Many VRP research papers credit TS (and its variants) with near state-of-the-art results ￼, so it is a proven approach for high solution quality.

(Note: The “Tabu Search for Quadratic Assignment Problem” is an example of TS’s adaptability. In QAP, TS uses swaps of facility assignments as moves. By analogy, for F-CVRP, TS can swap which customers are assigned to routes or even which family members are chosen – demonstrating that TS can manage the assignment aspect of F-CVRP effectively by analogous swap-based neighborhoods.)

Guided Local Search (GLS)

Applicability: Guided Local Search is a metaheuristic enhancement that sits on top of a local search procedure. It is very applicable to F-CVRP as a way to push a basic local search to explore new terrain. GLS modifies the objective function by adding penalties to certain solution features, thus “guiding” the local search out of local optima. It has been successfully used for vehicle routing problems (e.g. VRP with time windows) to improve upon pure local search ￼.

Strengths: GLS is relatively easy to implement and can lead to significantly better solutions than plain local search. It doesn’t require maintaining a population or complex memory structures – instead, it uses a simple penalty scheme to escape local minima. For example, if local search gets stuck using a particular set of expensive edges, GLS will impose a penalty on those edges so that the next local search iteration avoids them, encouraging a different route configuration ￼. In one study on VRP with Time Windows, adding GLS yielded 12 new best solutions on Solomon’s benchmarks, outperforming earlier heuristics in cases where routes were longer ￼. This shows GLS’s ability to systematically break out of traps that standard greedy improvement might fall into. It’s also a fairly generic method – you just need to define which features to penalize – making it adaptable to the family aspect of F-CVRP.

Weaknesses: The challenge with GLS is choosing the right features and penalty factors. If penalties are too strong or misassigned, the search might detour into poor regions or oscillate. Typically, GLS introduces a parameter $\lambda$ to scale the influence of penalties relative to the original objective. Tuning $\lambda$ is important: too low and GLS might not escape local optima; too high and it might ignore the real distance cost too much. Moreover, GLS relies on an underlying local search to do the heavy lifting – it doesn’t guide the search between very distant solutions as effectively as something like Tabu or SA might. It will systematically explore variations, but if the problem landscape has many local optima, GLS might need many iterations (re-penalizing repeatedly) to find a really good one. Implementation-wise, one must track the occurrence of features (like how often each edge is used) and update penalties, which adds a bit of overhead.

Adaptation to F-CVRP: We need to decide on “features” in the solution that GLS will penalize when they appear too strongly:
	•	For routing, a natural choice is edges (arc between two customers). GLS can add a small penalty to frequently used edges in local-optimum solutions. This encourages the next search iteration to try alternative paths. In F-CVRP, we can do the same. For example, if a certain long edge (distance-wise) is part of the current routes, penalize it to encourage the algorithm to find a route that avoids that connection.
	•	We can also define family-related features. For instance, a feature might be “customer A of family X is served”. If the search keeps picking a particular hard-to-service customer (maybe because initially it seemed fine, but it causes long detours), GLS could penalize that choice, nudging the search to serve a different member of family X next time. This would directly tackle the family selection aspect by not always sticking with the same combination.
	•	Capacity-related features are trickier (we usually don’t penalize based on load since capacity is a hard constraint that’s either satisfied or not), but we might penalize very high route loads if we notice the search always maxes out a truck and that leads to distance issues – although this is less common since using capacity fully is often good, not bad.

In practice, we would run a local search to a local optimum, then identify a set of features to penalize (commonly, all features present in the solution get a penalty increment, or specifically those that contribute the most to cost). Then we rerun the local search with the modified cost (original distance + penalties) to find a new solution, and repeat. Over successive iterations, GLS will diversify the search by dissuading the reuse of certain routes or selections. It’s an open-source friendly approach because it builds on the local search code and just requires tracking some counters and adjusting costs. For F-CVRP, GLS provides a controlled way to explore different family member combinations and routing patterns without completely random restarts: it learns which decision patterns might be causing the search to stagnate and explicitly penalizes them, thereby escaping local minima ￼.

Iterated Local Search (ILS)

Applicability: Iterated Local Search is highly applicable to F-CVRP and in fact has been shown to be very effective for this specific problem ￼. ILS is a metaheuristic where you repeatedly apply local search, but between each LS application you perturb the solution to jump to a different region of the solution space. It’s simpler than Tabu or GA in that it still deals with one solution at a time, but it has a strategy to escape local optima (the perturbation). For complex problems like F-CVRP that have multiple local optima due to different family selections, ILS can be a powerful yet implementable approach.

Strengths: ILS is conceptually simple and often finds very high-quality solutions. In a recent study, an ILS algorithm was developed for F-CVRP and it “efficiently obtained feasible solutions for instances that could not be solved optimally, improving the best known upper bounds within the time limit” ￼. This indicates ILS is not only fast but also capable of surpassing even some exact method results in limited time. The strengths of ILS come from balancing intensification (the local search phase, which thoroughly optimizes a given solution) with diversification (the perturbation, which prevents the algorithm from getting stuck permanently). It’s also flexible: one can adjust the perturbation strength (small shake vs. big shake) to explore more or less aggressively. Compared to Tabu, ILS has fewer parameters (no tabu list to manage, etc.) and compared to GA or ACO it’s easier to code because it’s just one solution evolving.

Weaknesses: The performance of ILS heavily depends on the quality of the local search and the perturbation method. If the local search is weak, ILS will just wander around suboptimal solutions. If the perturbation is not well-calibrated, two things can go wrong: (1) a too-small perturbation won’t effectively escape the current local optimum (you’ll fall back into the same local optimum repeatedly), or (2) a too-large perturbation will randomize the solution so much that you lose all the structure and essentially restart the search (wasting the work done so far). Tuning that balance is both art and science. Additionally, ILS does not inherently use a memory of past solutions (beyond the current one and maybe best-ever solution), so it might revisit similar solutions unless the perturbation process inherently avoids this. Sometimes, enhancements like a small pool of elite solutions (to restart from) or adaptive perturbation sizes are added to improve ILS – which starts to make it closer to other metaheuristics. Overall, pure ILS is somewhat simpler but might require careful thought to get the most out of it for F-CVRP.

Adaptation to F-CVRP: We design the components of ILS as follows:
	•	Initial solution: (We will detail strategies in the next section.) Start with a feasible solution (all families satisfied, capacities okay).
	•	Local search: Use a strong local search procedure combining all the relevant moves (route 2-opt, swap, relocation, and family-member exchange). This will give a locally optimal solution with respect to those moves.
	•	Perturbation: This is the key. A good perturbation for F-CVRP might involve changing the selection of some family members or reallocating some customers to different routes. For example, a perturbation could randomly remove a small set of customers from the solution (say 2–3 customers, ensuring that if we remove a customer, we mark that family as under-served temporarily) and then reinsert them (or others from their families) in a different way. Another perturbation could be to pick one or two whole routes and reshuffle them: drop all customers from those routes (making those families unserved) and then reassign the required number via a quick constructive heuristic. Essentially, we create a disturbance that the next local search will have to fix, hopefully leading to a different local optimum. We ensure the perturbation doesn’t violate constraints in an irreparable way – e.g. if we remove all 3 required customers of a family, we must add back 3 from that family during the reinsert phase.
	•	Acceptance criterion: Usually in ILS, you accept the new solution after local search even if it’s worse, with the idea that you’re exploring. However, often you keep track of the best solution seen overall. A common strategy is accept every new solution as the current one (to continue perturbing from), but if a perturbation+LS results in a worse solution, you might occasionally reject it or increase perturbation next time. Simplicity is key though: many ILS implementations just accept the new local optimum unconditionally and only store the best-ever solution separately.

By repeating this loop (LS -> perturb -> LS -> …), ILS will explore many locally optimal solutions. It leverages the speed of local search and adds the means to jump to new basins of attraction (new combinations of family members and routing structure). As cited above, this approach has been very successful for F-CVRP ￼, which suggests that with a well-designed neighborhood and perturbation, ILS can consistently produce high-quality solutions. It’s also feasible within a 5-minute run for moderately sized instances, since each local search round is polynomial in problem size and you can control how many iterations to attempt.

Adaptive Memory and Pool Methods

Applicability: Adaptive memory methods refer to keeping a pool of good solutions or solution components and reusing them to construct new solutions. This approach is very relevant to F-CVRP because of the problem’s combinatorial nature – good routes or good customer selections can be combined to form even better overall solutions. Techniques like Adaptive Memory Programming (AMP), scatter search, and Adaptive Memory Pools fall in this category ￼ ￼. They are often hybridized with algorithms like Tabu or ILS (for example, an “Adaptive Memory Tabu Search” stores elite solutions to draw from later ￼).

Strengths: Adaptive memory exploits the principle that high-quality solutions share useful substructures. By collecting “promising solution features” (e.g. particular routes that consistently appear in good solutions, or specific customer assignments that work well) and combining them, one can generate new solutions that inherit the best traits of multiple others ￼. This often yields very high quality solutions, as confirmed by various studies: for instance, an Adaptive Memory algorithm for a VRP variant managed to produce several new best solutions and was efficient on instances up to 400 customers ￼. The method tends to enhance diversity (since the pool contains different solutions) while also intensifying search around good regions (by recombining good parts). In practice, adaptive memory approaches are behind some of the top-performing VRP heuristics – e.g., the “Granular Tabu” mentioned earlier can be seen as a simple adaptive memory usage (keeping memory of good edges), and more explicitly, the Hybrid Genetic Search (HGS) by Vidal et al. (a leading CVRP solver) uses an elite pool of solutions and merges them to explore the search space, essentially an adaptive memory strategy. Another benefit: these methods are modular – you can often plug an adaptive memory module on top of an existing heuristic (like periodically take the best solutions found by Tabu Search and combine them via a crossover-like operation).

Weaknesses: The complexity of implementation is higher because now you need to manage a pool of solutions and a combination mechanism. If not done carefully, merging solutions can violate feasibility (e.g., mixing routes from two solutions might lead to too many from one family). So one often needs a repair or re-optimization after combination. Deciding which solutions to keep in the pool (how to update the memory) is also non-trivial – you might keep the top N best solutions, or some diverse set covering different structures. This adds another layer of parameters (pool size, selection criteria for memory usage). Computation time can increase if each combination requires solving a subproblem. However, since we are limited to 5 minutes, one would likely use the pool in a limited but strategic way (e.g., combine solutions when improvement has stalled, rather than continuously). Finally, adaptive memory on its own is not a standalone search (except in scatter search frameworks); it relies on other methods to feed it good solutions. It’s more of an enhancement strategy than an algorithm you run from scratch (though the literature sometimes calls the whole framework an “adaptive memory algorithm”).

Adaptation to F-CVRP: We can design an adaptive memory framework for our problem as follows:
	•	Pool of elite solutions: Maintain a list of the best (say) 5–10 solutions found so far, ensuring they are sufficiently different (to provide diverse material). Each solution includes the specific family members chosen and the routes.
	•	Combination operator: Develop a way to combine two (or more) solutions from the pool into a new, hopefully better, solution. For VRP, a common approach is to do a route combination: e.g., take some routes from Solution A and some from Solution B to form a partial new solution, then fill in the rest by solving a smaller problem for the leftover customers. In F-CVRP, we have to also ensure each family’s quota is met. So, if we take routes from A and B, we might end up with duplicate service of some families or missing some families. One strategy is merge by family: for each family, look at which members are served in A and in B. Construct the child solution’s family selection by picking the members that appear in either (or both) parent solutions most frequently – the idea is those were good choices in the parents. Once we have a fixed set of customers for each family in the child, we need to build routes for them (which can be done by a small VRP heuristic). Alternatively, we could merge at the route level with adjustments: include all routes from both parents, then resolve conflicts (if a family is over-served, remove one of the extra members, preferably the one causing more cost; if under-served, add a member that was missing, perhaps the one that appears in one of the parent solutions).
	•	Integration with search: We could run a metaheuristic like ILS or Tabu for a while, fill the pool with variants of solutions found, then occasionally pick two from the pool and combine them to see if we get improvement. The resulting combined solution can then be put back into the search (e.g., start a new Tabu Search from this combined solution) or directly placed into the pool if it’s good.

For example, scatter search does exactly this kind of thing: it generates a reference set of solutions and combines them systematically. In our context, after some initial solutions are found (via constructive heuristics and local search), we could combine them and then improve the offspring with local search. This adaptive memory approach ensures we are not limited to exploring around a single solution at a time – we effectively allow information to flow between multiple good solutions, creating new search trajectories ￼. This can help escape situations where one metaheuristic run might converge prematurely; the pool provides alternative good configurations to draw from. Given the complexity, one might implement this adaptivity in later stages of development, but conceptually it aligns well with achieving high solution quality for F-CVRP.

Simulated Annealing (SA)

Applicability: Simulated Annealing is a probabilistic single-solution search method inspired by the annealing process in metallurgy. It can definitely be applied to F-CVRP, as SA has been applied successfully to many VRP variants ￼ ￼. The algorithm is generic: it requires a way to generate neighbor solutions and a mechanism to accept or reject moves based on a “temperature” parameter. This generality means SA can handle the extra family-selection dimension by including those moves in its neighborhood. It’s a good option if we want a simpler metaheuristic that still escapes local optima by occasionally accepting worse solutions.

Strengths: SA is conceptually simple and robust. It was one of the first metaheuristics introduced for combinatorial optimization in the early 1980s ￼, and notably, it was first tested on the Traveling Salesman Problem ￼ – a classic routing problem – showing it could find near-optimal tours for large TSPs by proper cooling. The key strength of SA is the controlled randomization: at high “temperature” it performs a random walk that can jump out of local minima by accepting uphill moves (worse solutions), and as the temperature “cools”, it becomes more greedy/refined, homing in on a minimum. This process can theoretically find a global optimum given enough time and a slow cooling schedule. In practice, SA often finds very good solutions if tuned well. For VRPs, studies have shown SA can achieve competitive results; for example, Chiang and Russell (1996) applied SA to VRP with time windows using two kinds of neighborhood moves and achieved substantial improvements by carefully lowering the temperature ￼. SA’s implementation is easier than Tabu’s – no need for complex memory structures – and it can use the same local move set as a local search, just with a stochastic acceptance criterion. It’s also inherently able to handle continuous optimization problems in addition to discrete ones, using the same acceptance logic ￼. This generality means our implementation mindset can be uniform: treat the current solution’s cost as an “energy” and occasionally allow increases in energy to escape traps, which is a concept directly portable to F-CVRP.

Weaknesses: The main weakness of SA is that it can be slow to converge if not tuned properly. The cooling schedule (initial temperature, cooling rate, number of iterations per temperature, stopping temperature) has to be chosen with care. Too fast cooling will make it behave like greedy local search and potentially get stuck; too slow cooling yields better solutions but might take too long (beyond our 5-minute limit). Another issue: unlike Tabu which has a more strategic memory, SA’s moves are memoryless aside from the current state and temperature – it doesn’t learn from history except via the decreasing temperature. This means if the cooling is not ideal, SA might waste time exploring known-bad areas multiple times. SA also requires the definition of a neighbor generation that adequately explores the space. In F-CVRP, if the neighborhood is limited, SA might still have difficulty jumping between very different family selections unless we include moves that do that. In summary, SA gives us a framework to allow uphill moves, but we must calibrate probabilities (via temperature) such that it neither thrashes randomly nor freezes too early. There is also some parameter tuning involved (initial temp, cooling factor, etc.), which can be non-intuitive (though guidelines exist, e.g., set initial temperature so that a certain percentage of uphill moves are accepted at the start).

Adaptation to F-CVRP: Implementing SA for our problem would involve:
	•	State and neighbors: Use the same representation of a solution (routes + selected customers per family). Define the neighbor moves (2-opt, swap, relocate, family exchange – as described in LS section). Each move gives a new solution and a change in total distance cost.
	•	Acceptance rule: If the move yields a lower cost, accept it outright. If it’s higher (worse solution), accept it with probability $p = \exp(-\Delta / T)$ where $\Delta$ is the cost increase and $T$ is the current temperature. This means at high $T$, even quite worse moves might be accepted, whereas at low $T$, only very small increases have a chance.
	•	Cooling schedule: Start with a high temperature $T_0$. We can determine $T_0$ by, say, evaluating some random moves and setting $T_0$ such that about 50% of  moves with a moderate cost increase would be accepted. Then every few iterations (or every iteration), reduce $T$. A common schedule is geometric: $T_{new} = \alpha \cdot T_{old}$ with $\alpha$ close to 1 (like 0.99). We continue until $T$ is very low or a certain number of iterations have passed without improvement.
	•	Application to F-CVRP moves: SA will sometimes allow a move that breaks the current family selection or route structure even if it makes the solution worse temporarily. For example, it might accept a move that takes a convenient customer out and replaces with a slightly worse one if it allows subsequent moves to unlock a much better routing. This “two steps forward, one step back” process is valuable for F-CVRP because it might need to temporarily worsen the distance to switch which family members are being served, after which the route cost can be greatly improved. At high temperature, SA might even randomize the selection of an entire family’s customers (through a series of accepted worse moves) – essentially exploring a different combination. As temperature goes down, such drastic changes become less likely, and the focus turns to refining the routes for the chosen set.

In practice, an SA for F-CVRP would likely yield slightly inferior results to a well-tuned Tabu or ILS in the same runtime, based on general experience with VRPs. However, it is easier to implement and does reliably improve over greedy solutions. It’s an open-source friendly approach since it doesn’t need external libraries: just random number generation for acceptance and a loop. We will need to experiment with cooling rate and iteration count to ensure it finishes in time (for example, we might run SA for a fixed number of iterations or until a time limit). SA’s stochastic nature also means if time allows, we could run it multiple times and take the best solution, or use it in combination (e.g., an SA run could seed the initial pool of an adaptive memory approach). Summing up, SA provides a straightforward way to avoid local optima by occasionally accepting worse interim solutions ￼ ￼, and its adaptability to both discrete and continuous problems ￼ means it’s conceptually well-suited to the mix of selection and routing decisions in F-CVRP.

Ant Colony Optimization (ACO)

Applicability: Ant Colony Optimization is a population-based metaheuristic that is naturally suited for routing problems. It involves a number of “ants” constructing solutions guided by pheromone trails that encode learned desirability of using certain elements (e.g. edges). ACO has been extended from the classic TSP to VRP settings ￼. For F-CVRP, ACO can be applied, but it requires some adaptation because the ants must decide not only on routing but also on which customers from each family to include. Nonetheless, ACO frameworks have been used for VRPs with additional constraints (time windows, pickup-delivery, etc.) ￼, indicating it’s a viable approach here too.

Strengths: ACO benefits from collective learning and positive feedback. Each “ant” builds a solution from scratch, one customer at a time, probabilistically choosing the next customer based on a combination of a pheromone value (which reflects experience from past good solutions) and a heuristic value (often the inverse distance or other greedy desirability). Over iterations, good decisions (like using a short edge connection) accumulate higher pheromone, making future ants more likely to incorporate those sub-tours ￼. This autocatalytic behavior can lead to near-optimal solutions given enough iterations. ACO is also inherently parallelizable (ants work independently except for updating shared pheromones). For VRP, the seminal work by Bell and McMullen (2004) introduced an ant colony system that allowed constructing multiple routes (ants would start new routes when needed) ￼. Since then, improved versions of ACO for VRP have added features like specialized pheromone updates and local search phases for each ant’s tour ￼. ACO can handle capacity constraints by, for example, forbidding an ant from visiting a customer that would exceed capacity on the current route (the ant then starts a new route). It’s also friendly to combining with other methods; e.g., one can use a greedy or local search to post-optimize each ant’s tour after construction (this is known as the hybrid ACS with local search, often improving solution quality). In summary, ACO’s strengths are in exploring many solutions in parallel and gradually intensifying search around the best found routes through pheromone reinforcement ￼.

Weaknesses: ACO can be computationally heavy. Each iteration (with multiple ants) constructs several full solutions, which for a large problem can be slow if not optimized. Also, ACO typically needs a fair number of iterations for pheromone information to truly hone in on an optimum. In a 5-minute run, depending on instance size, we might be limited in how many iterations (and how many ants per iteration) we can execute. Another challenge: ACO is not straightforward to adapt to the family selection part of F-CVRP. In a standard VRP, ants visit every required customer exactly once. In F-CVRP, not all customers are required – ants have to effectively choose a subset of nodes (the required ones from each family) and decide routes for them. This is a more complex construction problem than a simple path through all nodes. It may require a custom approach, such as having the ants build a solution in two phases or encoding the selection within the pheromone on nodes. For instance, one could treat each family as a “macro-node” that must be visited a certain number of times, and each visit chooses a specific family member (which might involve pheromones on the choice of member). This is doable but adds complexity. Additionally, like GA, ACO has several parameters to tune (pheromone evaporation rate, initial pheromone, relative weight of pheromone vs. heuristic (alpha, beta), number of ants, etc.). If these are not set well, ACO might converge too slowly or get stuck in suboptimal habit (exploration vs exploitation balance).

Adaptation to F-CVRP: Here’s how we can adapt ACO for our needs:
	•	Representation: We need to extend the graph representation. One way: include all customers in the graph as potential nodes, but not all need to be visited. We can have a dummy depot node that must be visited at the start and end of each route as usual. We need to ensure exactly m customers from each family are visited in total. One approach is to enforce this via a constraint check at the end of solution construction (and discard or heavily penalize any ant’s solution that doesn’t meet it). A more directed approach is to integrate selection into the construction: for example, an ant when at the depot deciding to start a new route might probabilistically pick a family and a member of that family to visit next, ensuring that over the full construction it picks exactly the required number from each family. This might require a counter for each family that the ant decreases whenever it serves a member. The ant’s tour-building state thus includes how many members of each family remain to be served. This statefulness makes the problem of pheromone definition harder (because the desirability of going to a particular customer might depend on how many of their family have already been served).
	•	Pheromone scheme: We can maintain pheromones on edges between specific customers (like in TSP and standard VRP) ￼. That would encourage ants to use certain connections that in past solutions led to good outcomes. In addition, we might maintain pheromones on node selection for families. For example, for each family, have pheromone values on each member indicating how often choosing that member contributes to good solutions. This would be akin to a “selection pheromone”. An ant constructing a solution could then use two levels of choice: which family to pick next (or which specific customer next) and which particular member if that family still needs serving. However, to keep it simpler, we might avoid a two-level pheromone and just encode everything in pheromone on actual customer-to-customer moves (the presence of certain edges indirectly encourages certain nodes being in the tour).
	•	Feasible solution construction: We enforce capacity by having the ant end the current route and start a new one whenever adding the next customer would exceed capacity. We enforce family counts by tracking how many from each family have been chosen so far. If a family already has its quota served, the ant should consider remaining members of that family as effectively “not available” (or have zero probability to be chosen further). If a family still needs service, its members (that are not yet visited) remain candidates. The ant finishes when all families’ required members have been visited (which might mean some customers are left unvisited entirely, which is fine). If an ant prematurely ends because it closed routes and still some families not fulfilled, that solution is invalid; we could either discard it or repair it (e.g., if an ant fails to include all needed, maybe have a heuristic fill-in after the fact).
	•	Pheromone update: After all ants construct solutions in an iteration, we update pheromones. Typically, we evaporate a percentage of pheromone on all edges (to forget poor decisions over time) and then add pheromone on edges that were used in the best solutions found this iteration (or overall best). We may deposit more pheromone for better solutions (proportional to 1/cost). This way, good routes (edges) get more attractive for future ants ￼.
	•	Heuristic information: We will have a heuristic preference for shorter distances and perhaps for serving required customers that are “hard” (like far away) earlier. A simple heuristic is the inverse distance: an ant at customer A will have a heuristic value $\eta_{A,B} = 1/\text{dist}(A,B)$ for moving to customer B next (higher if closer). We might also incorporate a heuristic that favors visiting still-unserved families (for example, give a small bonus for going to a customer of a family that still needs many members served, to ensure those get picked up sooner rather than leaving all for the end).

Despite the complexity, there have been successful demonstrations of ACO on VRP variants with additional constraints ￼. For instance, researchers have combined ACO with local search to solve VRPs with pickup and delivery and time windows. For our project, implementing a full ACO from scratch is a larger effort. It might be something to attempt after we have a simpler metaheuristic working, due to time constraints. However, it’s worth noting that ACO is an open-source metaheuristic (numerous papers and implementations exist) that could potentially find very good solutions if we manage the selection constraint carefully. If done well, ACO would produce a pool of solutions each iteration (the ants’ solutions), so we get a natural diversification, and the best solution improves over iterations as pheromones converge to favor optimal routes ￼. The downside is ensuring convergence within 5 minutes might require fine-tuning the number of ants and iterations (e.g., using a moderate number of ants like 10–20 and maybe a few hundred iterations, depending on problem size). We should also be mindful to include at least a rudimentary local improvement for each ant’s finished tour (like a quick 2-opt on each route) to polish the solutions before pheromone update – this is common in ACO applications and greatly improves solution quality for a given number of iterations.

Evolutionary Algorithms (Genetic Algorithm)

Applicability: Evolutionary algorithms, particularly Genetic Algorithms (GA), are a popular choice for routing problems and can be applied to F-CVRP with tailored encoding. A GA maintains a population of candidate solutions and evolves them through selection, crossover (recombination), and mutation operators analogous to biological evolution ￼. Many researchers have applied GAs to VRP (and TSP) and found them effective when combined with domain-specific tweaks. For F-CVRP, a GA would need to encode both which customers are selected (family aspect) and the routing of those customers. This dual aspect makes the encoding and crossover design a bit complex, but there are established methods (like giant-tour representations or two-part chromosomes). GA (or evolutionary strategies in general) is applicable especially when we want to explore a wide variety of solutions and take advantage of recombining good traits from different solutions.

Strengths: GAs are good at exploration and combining partial solutions. Crossover allows two good solutions to produce an offspring that maybe inherits the best of both (for example, some efficient routes from one parent and some from the other). Over generations, the population tends to improve as good traits propagate. GAs also inherently maintain diversity (especially if the population is well managed), which helps avoid getting stuck in one region of the search space. For instance, different individuals can represent different ways of choosing family members, so the GA can simultaneously search multiple selection combinations. There are well-known crossover operators tailored to routing problems (Order Crossover, Partially Mapped Crossover, Edge Recombination, etc.) ￼ that try to preserve relative order or adjacency of nodes, which is important in VRP/TSP. Mutation operators (like randomly swapping two customers in a route, or replacing one family member with another) ensure new material enters the population. Genetic Algorithms for TSP have been studied since the 1980s and shown to be competitive; similarly for CVRP, GAs (especially when hybridized with local search) have achieved excellent results ￼. In fact, modern memetic algorithms (GA + local improvement) are regarded as state-of-the-art for VRP: a hybrid GA known as the Hybrid Genetic Search or Memetic Algorithm has been “the most competitive vehicle routing heuristic” in recent years ￼, blending evolutionary search with strong local optimization. This indicates that an evolutionary framework, if augmented with problem-specific operations, can yield very high solution quality. GAs are also relatively straightforward to parallelize or run multiple independent trials of, since populations can be split or different runs can exchange individuals.

Weaknesses: The main challenge is the encoding of solutions and maintaining feasibility. A VRP solution is a set of sequences (routes) and we also have the selection constraint. A naive encoding might be very long or require repair. For example, one common encoding for VRP is the “giant tour” representation: represent a solution as a single sequence of all served customers (like a TSP tour) and cut this sequence into routes at certain breakpoints (this can be encoded by special separator symbols or by a list of route break indices). This representation can be crossed over using TSP techniques, but ensuring capacity and family counts are satisfied after crossover may require a repair step (e.g., if the child’s routes violate capacity or serve wrong number from a family, we must fix it, which can be complex). Another encoding is two-part: one part of chromosome decides which customers (one per family) are selected, the other part orders them into routes. Designing crossover for a two-part representation such that both parts remain consistent (the route part should only contain the selected customers) is tricky. Often, researchers avoid splitting it explicitly and instead rely on repair: allow crossover to potentially mess up the selection, then correct it by adding/dropping customers to meet the family quotas. This adds computational overhead and might degrade solution quality if not careful. GAs also have many parameters: population size, crossover probability, mutation rate, etc., which need tuning. If population size is too small or too large for the time given, performance suffers (too small -> premature convergence; too large -> slow evolution). Also, pure GA (without local search) might take many generations to reach the fine-tuned solutions that local search or TS could find quickly, because crossover by itself doesn’t do granular improvements — it mainly rearranges large parts. That’s why the best VRP GAs incorporate local search on individuals (making them memetic algorithms). For our time budget, a GA without any local search might not achieve the absolute best quality compared to Tabu or ILS, but a GA with local improvement could come close at the cost of additional implementation complexity. Finally, one must maintain diversity; sometimes a GA can converge prematurely (population becomes too similar), which can be mitigated by occasional random immigrants or diversity-preserving selection, but that’s additional logic to consider.

Adaptation to F-CVRP: We have options for encoding:
	•	Direct (route list) encoding: Represent a solution as a list of routes, where each route is a sequence of customer IDs. We could concatenate these routes into one list with special delimiters for crossover purposes. Two parent solutions can be crossed by something like the Unified Routing Crossover (URX) or route exchange: e.g., randomly choose some routes from parent A to give to child, and fill the rest with routes from parent B (removing duplicates and fixing missing families as needed). This is similar to how one might combine VRP solutions by routes (which is analogous to the adaptive memory combination earlier, but done within GA).
	•	Giant-tour with split: Represent the solution as a permutation of the served customers (length equals total customers to be served across all families) and use a split algorithm to divide into routes optimally (respecting capacity). The genetic operators act on the permutation (like a TSP). This is nice because any permutation of the required customers can be decoded into a feasible routing by a separate procedure (like a dynamic programming that minimizes distance given that sequence and vehicle count – commonly used in GA for VRP). The twist here is we also need to know which customers are in that permutation. We could fix that by always having the permutation contain exactly m members of each family, so the selection is inherently encoded. But how to ensure that in crossover? Possibly enforce that initial population all have valid selection (they will), and then restrict crossover in a way that maintains those counts: e.g., use specialized crossover that treats genes family-by-family. For instance, do crossover in two phases: first decide which family members the child inherits from which parent (for each family, maybe choose one parent’s selection with some probability), then within each family’s chosen members, use an order crossover to arrange them in the child’s permutation respecting the order they had in the parents. This ensures the child has valid family selections. This is a custom operator we’d have to write.
	•	Two-chromosome approach: One chromosome bit string for selection (for each family, which members are chosen) and one for routing order. But this is problematic because the routing part only makes sense relative to who is selected. If a mutation flips the selection, the routing part might refer to a customer that is no longer selected. So it requires heavy synchronization logic or repair, hence not ideal.

Given the complexity, a pragmatic approach is: use a GA mainly to shuffle and exchange routes and selections at a coarse level, and rely on local search to clean up. For example, one could encode just the selection (which members of each family) in the GA chromosome and have a fixed routing heuristic to decode it (like whenever you evaluate a chromosome, you know which customers are selected, then run a routing heuristic to build the actual routes and get the cost). Crossover would swap portions of this selection information between parents. This way, GA is exploring the family selection space, while a heuristic handles the VRP solution for that selection. Alternatively, one could fix selection (if maybe that’s predetermined) and use GA just for routing (like a standard CVRP GA). But usually in F-CVRP, selection is part of the optimization.

To incorporate known best practices: we should integrate local search on offspring (this turns the GA into a memetic algorithm). For each child produced by crossover, after repairing feasibility, run a quick local search to optimize the routes (and possibly tweak the selection). This greatly improves the fitness of each individual and speeds up convergence ￼. It does add computational cost per generation, so the population or generations count might be reduced accordingly.

In terms of open-source implementation, a GA doesn’t require external libraries; we can code the selection, crossover, mutation ourselves. We will likely have a population of maybe 20–50 solutions, run for perhaps tens of generations within 5 minutes (depending on local search cost). GAs have been used in the literature (e.g., Baker and Ayechew 2003 developed a GA for CVRP that achieved good results ￼), but the consensus in recent years is that GAs alone are outperformed by hybrids (memetic algorithms) ￼. Therefore, if we pursue an evolutionary route for F-CVRP, we should incorporate problem-specific knowledge (like the custom crossovers and LS) to ensure solution quality. The payoff is potentially very high-quality solutions, since the combination of population search and local optimization is known to yield state-of-the-art results for many VRPs ￼.

⸻

Having compared these algorithms, we can summarize: Local search is a must for improving any candidate solution; Tabu Search and Iterated Local Search stand out as particularly well-suited for F-CVRP (they have the right balance of exploration and exploitation, and literature evidence suggests their efficacy ￼). Guided LS and SA offer simpler implementations that can still escape local optima, possibly at the cost of more tuning. ACO and GA/EA are powerful but require more complex adaptation for the family aspect and careful parameter tuning; when hybridized with local search, they could achieve the best results (memetic approach) but they are more involved to implement within our constraints.

Next, we outline a practical plan to implement an initial solution and then iteratively improve it using the best-fit techniques identified.

Step-by-Step Solution Implementation Plan

To tackle the F-CVRP effectively, we propose the following step-by-step strategy:
	1.	Data Preparation and Representation: Parse the input (customer locations, families, vehicle capacity, etc.) into a convenient data structure. For example, store customers as objects or indices with attributes: family ID, demand (if any, though here it might be 1 for each if just one person deliveries), coordinates, etc. Precompute a distance matrix dist[i][j] for travel distances between all customer locations (including the depot) to allow O(1) cost lookup for any potential route edge. This preprocessing will speed up route evaluations significantly. Also, keep track of family groupings (e.g., a dictionary mapping family ID to the list of customer IDs in that family, and how many must be chosen from each).
	2.	Initial Family Member Selection: Construct an initial feasible set of customers to serve, selecting the required number from each family. A simple and effective heuristic is:
	•	For each family, rank its members by some criterion, for instance their distance to the depot (closer might be better as they are easier to route) or a combination of distances to other nearby customers (to favor clustering).
	•	Select the top required number for service. This ensures each family is represented and tends to pick “easier” customers first (though not always optimal, it’s a reasonable start).
This gives us the subset of customers we will actually route. (If the problem required deciding the number to serve, it would be harder, but here it’s fixed per family.)
	•	Optional: One could also use a randomized selection for diversity (pick some families’ members at random to get a different starting point and run the algorithm multiple times). But initially, a deterministic heuristic like nearest-to-depot should suffice.
	3.	Initial Routing Solution Construction: Now given the chosen set of customers (let’s call this S), we need to create vehicle routes to cover all of S (each exactly once) without exceeding capacities. We know the total number of customers is sum(required from each family), and the number of vehicles available (from the input). Use a classical constructive heuristic for CVRP:
	•	One common open-source method is the Clarke-Wright Savings algorithm ￼. This starts with each customer on a separate route (or each family selection on separate routes) and then iteratively merges routes if it saves distance and capacity allows. We compute a savings value for each pair of customers i, j (roughly: dist(depot,i)+dist(depot,j) - dist(i,j)). High savings means it’s good to connect i and j on the same route. Then we merge routes greedily in order of savings.
	•	Another method is a greedy nearest neighbor or sweep algorithm: e.g., sort customers by polar angle around the depot (sweep) and partition into routes by capacity, or start at depot and keep adding the nearest feasible customer to the current route until you can’t add more (then start a new route).
	•	Either way, ensure that no route exceeds capacity. If a vehicle capacity is 400 (like in the example file) and each customer has a certain demand (say each demand = 1 or another value), track the cumulative demand on the route as you add customers. If adding a customer would exceed 400, start a new route.
	•	The result of this step is an initial solution: a set of routes covering all families’ requirements, likely not optimal but feasible.
	4.	Local Search Improvement: Apply a local search to improve the initial routing:
	•	Implement a function for cost calculation of the entire solution (sum of distances of all routes). Also functions to evaluate a potential move’s cost change (for efficiency).
	•	Use intra-route improvement first: for each route, perform 2-opt swaps to eliminate crossing paths and shorten the route (this can be done quickly because it’s just within one route’s customer sequence). Keep doing 2-opt until no more improvement in any route.
	•	Then inter-route improvements: try relocating a customer from one route to another if both capacity and family counts remain OK (family counts will remain OK because we aren’t changing who is served, just which route serves them). Do this in a loop or using a heuristic like iterative improvement: iterate over all customers, try moving them to any other route at the best position, accept the move if it reduces cost. Also try swapping pairs of customers between two routes (this can balance route lengths).
	•	Throughout, maintain the feasibility: if a move violates capacity, skip it. (Swapping might actually help capacity balance sometimes, so consider it if one route is heavy and another light).
	•	Also incorporate the family exchange move: for each family, if it currently has a set of specific members served, try replacing one of them with another member of the same family (that was not served) and see if the routes get shorter. This requires removing one customer and inserting the alternate one either in the same position or elsewhere. You’d have to evaluate how the new customer fits in (maybe try inserting them in the route of the removed customer and see cost difference). This is a larger neighborhood move, so maybe do it less frequently or after other improvements stagnate.
	•	Use an iterative approach: go through different move types repeatedly until no improvement is found in a full pass (this is a classic VRP local search approach known as VND – Variable Neighborhood Descent, where you cycle through move neighborhoods). Because a local optimum for one type of move might not be for another, cycling helps ￼.
	•	After this, you have a locally optimized solution that should be significantly better than the initial.
	5.	Metaheuristic Enhancement: Now we introduce a higher-level search to escape the local optimum and explore more of the solution space:
	•	Option A: Iterated Local Search (ILS). Take the current solution, apply a random perturbation: e.g., remove a few customers from their routes (just take them out, leaving those routes a bit incomplete) and also possibly swap one or two family selections (as described earlier, maybe drop one served member of a family and replace with an unserved member). Then run the same local search routine to re-optimize. This will likely result in a different local optimum. If it’s better than the best solution seen so far, record it. Repeat this process (perturb -> local search) for many iterations or until time runs out. You can have a strategy like: always accept the perturbed solution’s outcome (even if it’s slightly worse) to continue the exploration, but keep track of the best. Or use an acceptance criterion akin to simulated annealing (start by accepting worse results with some probability and decrease that over time to focus in). ILS can be quite effective and was proven efficient for F-CVRP instances that exact methods couldn’t solve ￼.
	•	Option B: Tabu Search. Implement a loop that at each iteration considers all (or many) neighbor moves (the moves here include all from the local search step, plus perhaps directly swapping family members as a neighbor move) and chooses the best move that is not tabu (or that satisfies aspiration if it is tabu). Maintain tabu lists for recent moves or inversions (e.g., if we moved customer X from route1 to route2, tabu moving X back to route1 for  Tenure iterations). Each time a move is made, apply it (even if it’s worse than current solution, as long as it’s the best non-tabu). Keep track of the best solution ever. Stop when iterations exceed a limit or no improvement has been found for a while. Tabu search will systematically search a broader neighborhood than greedy LS. Make sure to implement efficient move evaluation (store route costs, etc., so you can calculate cost changes quickly). Given a 5-minute runtime, you might perform thousands of moves – tabu is heavy but if the neighborhood is pruned (using the “promising moves” idea, e.g., only consider moving to routes that save a certain amount or edges below a certain length), it can be manageable ￼. The result should be an improved solution beyond the local optimum.
	•	Option C: Simulated Annealing metaheuristic. This can use the same neighborhood moves as Tabu, but instead of tabu lists, use probabilistic acceptance. Start at a higher “temperature” and randomly pick a move from the neighborhood (or even do a small number of random moves each iteration). Evaluate it; if it’s better, accept, if worse, accept with probability exp(-Δ/T). Decrease T gradually. This method is easier to implement but might require tuning the cooling schedule so that within 5 minutes it cools down sufficiently.
	•	Option D: Hybrid or others. We could also incorporate Guided Local Search at this meta level by penalizing features once a local optimum is reached, then rerunning LS (this is a bit like iterated local search where the perturbation is systematically guided by penalties). Or even a small Genetic Algorithm: maintain a set of a few solutions (maybe the best from different perturbations), and occasionally mix them (e.g., take half routes from one and half from another and then run local search to heal). This is essentially a memetic approach combining elements of GA and LS.
Choice of algorithm: Based on our analysis, a strong approach would be Iterated Local Search or Tabu Search (or a combination, like an ILS that uses a short-term tabu list during LS to diversify moves). ILS is easier to code – we can start with that. Tabu could yield slightly better exploration if well-tuned. We might implement one and then later try the other and compare. Both are open-source friendly (no libraries, just algorithms we code). We can also incorporate Adaptive Memory in a simple way: keep the best solution found in a global variable, and maybe maintain a list of top 5 solutions found; if after some iterations no new best is found, we can restart the search from one of the other top solutions (this is like a mild form of memory diversity).
	6.	Termination: Ensure the algorithm stops within the allowed time (5 minutes). We can use a loop that breaks either after a certain number of iterations or when a time/check limit is reached. Typically, you might say “run ILS for 1000 iterations or until 2 minutes of no improvement” or simply run until the clock is near 5 minutes. Since we want the best quality, we will likely use the full time budget to keep refining.
	7.	Result Output: After termination, output the best solution found (the routes and which customers are served). Also output the total distance. This solution should be feasible (by construction of moves) and of high quality due to the improvements.

This step-by-step process starts simple (greedy selection and routing) and incrementally adds sophistication (local optimization, then metaheuristic search) – a logical development path. It ensures we always have a valid solution at each stage and that the solution quality is non-decreasing.

Throughout implementation, we can test on small data to verify correctness (e.g., a mini F-CVRP with 2 families, etc.) before scaling up. By following this plan, each stage’s correctness (feasibility) can be verified, and each subsequent stage should only improve the objective. Importantly, this approach is modular, allowing swapping or tuning of the metaheuristic component (for example, one can try SA vs. Tabu vs. ILS easily if the local search and solution representation are well-defined).

Modular Python Project Structure

To manage this implementation clearly, a modular design in Python is recommended. Here’s a suggested structure with functions and components:
	•	Data Classes / Structures:
Define classes or namedtuples for fundamental entities:
	•	Customer with attributes: id, x, y, demand, family_id.
	•	Family with attributes: id, members(list of Customer ids), required_count. (This can be derived from input; e.g., family required_count might be uniform or specified).
	•	Route which contains a list of customer IDs in visit order (not including depot implicitly, we treat depot as a special node).
	•	Solution containing a list of Route and perhaps a set or list of served customer IDs for quick membership checking, plus the total cost. (We can compute cost on the fly or store it; storing and updating is useful for efficiency.)
	•	Utility Functions:
	•	dist(i, j): returns distance between two customers (use the precomputed matrix or calculate Euclidean if needed).
	•	calculate_route_cost(route): sum up distances from depot -> first customer -> … -> last customer -> depot for a given route.
	•	calculate_solution_cost(solution): sum of route costs. (This might not be needed if we maintain incremental updates, but good for verification.)
	•	check_feasibility(solution): verifies that each family’s served count is correct and no route exceeds capacity (for debugging mostly, as our moves will maintain feasibility).
	•	Initial Solution Construction:
	•	select_initial_customers(customers_by_family) -> served_customers: implements step 2 above. For each family, pick required_count customers (by heuristic or random). Return a list or set of selected customer IDs.
	•	construct_initial_routes(served_customers) -> Solution: implements step 3. Use Clarke-Wright or another heuristic. Possibly have a helper like savings_algorithm(selected_customers) -> routes. Alternatively, a simpler greedy: greedy_routes(selected_customers) -> routes.
	•	Possibly provide an option to choose which construction heuristic via parameters or try multiple and pick the best initial.
	•	Local Search Operations:
Implement a suite of neighborhood move functions that can apply to a solution:
	•	two_opt(route) -> improved (bool): perform 2-opt on a single route (return True if improved). This can be called in a loop for all routes until no improvement.
	•	relocate_customer(solution) -> improved: try moving a customer from one route to another. Probably best implemented as: iterate through all routes A and all customers in A, and all other routes B (including possibility of creating a new route if vehicles remain) – evaluate cost change of removing customer from A and inserting in B at best position. If any improvement found, do the best move.
	•	swap_customers_between_routes(solution) -> improved: choose two customers from two different routes and swap them. Ensure they are not from the same family (or if they are, it doesn’t change family count, which swapping same-family members wouldn’t, but usually you swap different ones). Evaluate cost delta.
	•	swap_family_member(solution, family_id, cust_out, cust_in) -> improved: a move to replace one served member of a family with another unserved member. This affects two routes: the route that had cust_out (from which we remove it) and the route (if any) that cust_in will go into (possibly the same route or maybe we insert cust_in into the route of cust_out to minimize disruption, or consider other routes too). We need to evaluate cost difference and capacity impact (likely similar demand, so capacity usually unchanged if demand is uniform). Implementing this might involve trying for each family and each currently served member, one of the not-served members, and finding best improvement.
	•	We should design these so that they just test and possibly return one move rather than fully apply multiple times, because we might integrate them in a larger loop or metaheuristic. Alternatively, implement an integrated local search procedure:
	•	local_search(solution) -> Solution: which internally calls these moves repeatedly until no improvement (this could be a while loop that goes through moves in sequence, setting a flag if any move was made). This function returns a locally optimized solution.
	•	Metaheuristic Controllers:
We can have different functions depending on which meta approach we use:
	•	iterated_local_search(initial_solution) -> Solution: implements the ILS loop. It would take an initial solution, set it as current best, then for i in 1..MaxIter (or until time):
	•	perturb = perturbation(current_best)
	•	local_opt = local_search(perturb)
	•	if local_opt cost < best cost: update best = local_opt
	•	possibly update current_best = local_opt (with some acceptance criterion if needed, or always).
	•	maybe occasionally if stuck, do a heavier perturbation.
	•	tabu_search(initial_solution) -> Solution: implements TS. We will need a tabu list structure (could be a list of tabu moves or a matrix marking recently moved customer-route assignments, etc.). Pseudocode:
	•	current = initial_solution, best = initial_solution
	•	initialize tabu_list
	•	for iter in 1..MaxIter: generate candidate moves (or a subset) and resulting costs; pick best allowable (not tabu or satisfies aspiration) move; apply it to current to get new_current.
	•	if new_current cost < best cost: best = new_current
	•	update tabu_list with the move (e.g., mark reversing that move as tabu for X iterations).
	•	if no move found (shouldn’t happen if we allow worse moves always unless all are tabu), break.
	•	return best.
	•	Because implementing full neighborhood each iteration is expensive, we might combine this with some neighbor filtering (e.g., only consider moving a customer if it yields at least some small gain or if it’s from a heavily loaded route to a lightly loaded route, etc., to cut down combinations).
	•	simulated_annealing(initial_solution) -> Solution: implement SA loop.
	•	current = initial_solution, best = initial_solution.
	•	T = T_start
	•	while T > T_min (or iter < MaxIter): pick a random move (or a random neighbor solution perhaps by randomly choosing a move type and random customer(s) involved). Let new_sol be that neighbor.
	•	If new_sol is infeasible, skip (or fix and evaluate cost).
	•	Δ = new_sol.cost - current.cost. If Δ < 0 (improvement), accept (current = new_sol) and if cost better than best, best = new_sol. If Δ > 0, accept with probability exp(-Δ/T). If not accepted, you can either stay with current or some implementations still set current = new_sol but mark it as not fully accepted (but typically you stay with old current if not accepted).
	•	reduce T = alpha * T (like 0.995 * T) periodically (each iteration or every N iterations).
	•	return best at end.
	•	This is easier to code but the challenge is ensuring you sample moves broadly (maybe randomly picking two routes and swapping, or random perturb like removal insertion as the neighbor each time).
	•	genetic_algorithm(population) -> Solution: (If we choose to implement GA)
	•	initialize population (maybe by random selections of family members + greedy routing, ensuring all feasible).
	•	For gen in 1..MaxGen or time:
	•	evaluate fitness of each individual (we have cost; smaller cost = higher fitness).
	•	select parents (tournament or roulette based on fitness ranks).
	•	crossover parents to produce offspring (need crossover function that handles our encoding and yields a valid or repairable child). Possibly produce as many offspring as population size.
	•	mutation: for each offspring, with small probability, make a random tweak (e.g., swap two customers in the route, or change one family selection and re-route).
	•	repair any infeasible offspring (if route over capacity or family count off, fix it by swaps or local repair).
	•	optionally apply local_search on offspring to improve them (memetic enhancement).
	•	form new population (either just offspring (generational) or offspring + some old (elitist)). Ensure best solution carries over or at least is not lost.
	•	track best solution overall.
	•	return best.
Given time constraints, we might not implement all of these. We can start with one (say ILS) and ensure others can be plugged in if needed since they use the same data structures. The design is to separate the neighborhood operations (which are in local search functions) from the high-level strategy (which is in ILS/Tabu/SA functions). This modularity means if one approach isn’t giving good results, we can switch to another by just calling a different function, without rewriting how solutions are represented or mutated.
	•	Main script / orchestration:
	•	Parse input file to get data.
	•	Build initial solution (sol0 = construct_initial_routes(select_initial_customers(families))).
	•	Run improvement metaheuristic of choice (e.g., best = iterated_local_search(sol0)).
	•	Print or return the best solution and its cost.
	•	Include timing checks if needed (e.g., break out of loops after 5 minutes).
	•	Logging/Debugging (optional):
	•	It’s helpful to have a way to output the current best cost at intervals, to see progress.
	•	Also, verify constraints at critical points (assert that each family count is correct after each major operation).
	•	Possibly a visualization or print of routes for small cases to ensure correctness.

This modular approach will result in cleaner, more maintainable code. Each function can be tested individually (e.g., test that two_opt actually shortens a route correctly, test that construct_initial_routes yields feasible solution, etc.). Also, if we want to try different algorithms, we mostly modify the high-level loop and maybe some parameters, rather than the entire codebase.

The code will avoid external libraries – we won’t use specialized VRP solvers or even heavy numpy usage (except maybe for distance matrix for convenience). Plain Python is sufficient given problem sizes (though if performance is an issue, we might optimize inner loops carefully or consider using simple numpy array operations for distance lookups). Ensuring the code runs within 5 minutes will involve careful consideration of how many iterations or how broad the neighborhood is, and possibly adding early## Parameter Tuning and Avoiding Local Optima

Designing the algorithm is only part of the challenge – proper parameter tuning and strategies to escape local optima are crucial for solution quality. Here are some tips and best practices:
	•	Algorithm Parameter Tuning: Each metaheuristic has key parameters that influence its performance. It’s advisable to fine-tune these on smaller instances or via experimental runs:
	•	Tabu Search: Tune the tabu tenure (the length an attribute stays tabu). A common strategy is a tenure around 5–15 moves for VRP-sized problems; too short and TS may revisit cycles, too long and it may overly constrain the search. You can also use a dynamic tenure that changes based on progress. Incorporate aspiration criteria (e.g., allow a tabu move if it yields a new global best) to prevent overly strict tabus. Monitor improvement: if TS isn’t finding better solutions for many iterations, consider increasing tenure (to diversify more) or injecting a random move.
	•	Simulated Annealing: Key parameters are initial temperature, cooling schedule, and iteration length at each temperature. Start with a temperature high enough that a significant fraction (perhaps 50%) of uphill moves are accepted initially – this might be determined by testing a few random moves’ $\Delta$ costs ￼. Use a cooling rate (alpha) in the range 0.95–0.99 per step; a slower cooldown (alpha closer to 0.99) gives SA more chance to explore but takes more iterations. Ensure the temperature is lowered to near-zero by the end of the run (so the final solution is fully “frozen” in a local minimum). If SA converges too early (temperature too low quickly), restart with a higher initial temperature or slower cooling. If it’s too random for too long, accelerate cooling slightly.
	•	Guided Local Search: The penalty weight $\lambda$ that balances original cost vs. penalties is important. A typical approach is to set $\lambda$ such that when a feature is penalized, it causes a noticeable but not dominating increase in cost (maybe around 5-10% of the base cost). Experiment with different $\lambda$ to see which yields consistent diversification. Also decide how often to penalize (e.g., only when stuck in a local optimum). The features chosen to penalize can be tuned – perhaps penalize the top few longest edges in the solution each time.
	•	Iterated Local Search: The perturbation strength (how many and which changes you make) is the main “parameter”. If the perturbation is too mild, ILS will revisit the same local optimum repeatedly; if too harsh, it’s essentially a new random start. A good rule is to disturb a solution just enough to escape the current basin. For example, remove 2–5 customers (out of, say, 100) for re-insertion. You can even randomize the perturbation size a bit each time. Monitor the outcomes: if ILS often returns to the same solution, increase perturbation; if it seems to randomize excessively, decrease it. Also, the number of ILS iterations to try can be set based on time – you might aim for, say, several hundred perturbation cycles within 5 minutes.
	•	Ant Colony Optimization: Important parameters include number of ants (colony size), pheromone evaporation rate, and the pheromone vs. heuristic influence (alpha, beta). A moderate evaporation (e.g. retain 0.5–0.8 of pheromone each iteration) ensures balance between forgetting and reinforcing. If evaporation is too low (pheromone accumulates), you risk premature convergence; if too high, ants behave almost randomly even after many iterations. Set alpha (pheromone weight) and beta (distance heuristic weight) such that beta > alpha usually (to guide ants by distance initially, but let pheromone take over as solutions improve). For example, alpha = 1, beta = 2 is common. Too many ants can slow each iteration without much benefit – often setting number of ants = number of customers (or a fraction of that) is enough. You might start with 10–20 ants and adjust. Check if the best solution is improving generation by generation; if not, consider increasing exploration (e.g., higher evaporation = more randomness, or add some randomness to pheromone updates).
	•	Genetic Algorithm: Key parameters are population size, crossover rate, and mutation rate. A population of around 30–50 can be a good balance for a VRP (ensuring diversity but not blowing up computation). Crossover probability is usually high (80% or more of new offspring are bred via crossover) because we rely on recombination to mix good traits. Mutation rate should be relatively low per individual (5–10% chance to mutate a given gene or a route) so that we don’t randomize solutions too much – the role of mutation is just to inject occasional new genes and prevent convergence. If you see the population converging (solutions becoming too similar), you can increase mutation rate or use a diversity mechanism (like if two individuals are identical, mutate one). Including local search in GA (making it memetic) means each generation takes longer, so you might reduce the number of generations accordingly – but those individuals will be of much higher quality, so you can afford a smaller population or fewer generations. It’s often effective to use elitism (carry the best few solutions unchanged to the next generation) to ensure quality doesn’t degrade.
	•	General Tuning Approach: It often helps to use an automated tuning tool or script to try different parameter combinations on sample instances. Researchers also use self-tuning methods that adjust parameters on the fly ￼ – for example, gradually decrease tabu tenure if no improvement is found, or adjust SA’s cooling if progress is slow. Since our implementation is custom, we can instrument it to collect stats (e.g., how often a certain move yields improvement) and adjust accordingly in future runs.
	•	Avoiding Local Optima: All the metaheuristic frameworks above are designed to avoid being trapped in poor local optima, but here are additional strategies to ensure we explore enough of the search space:
	•	Multiple Starts: Run the entire algorithm multiple times from different initial solutions (e.g., different random initial family selections or randomized route constructions) and take the best result. This is a simple way to inject diversification. For example, you could do 5 runs of ILS with different initial random seeds for selection or initial route order.
	•	Diversification within Search: Incorporate explicit diversification steps. Tabu Search can include a long-term memory that penalizes overused routes or encourages selecting rarely used customers (similar to GLS concept) to force the search into new areas after many iterations ￼. In ILS, if after N iterations no new best is found, perform a heavy perturbation (e.g., reshuffle 10 customers or swap out one member in several families at once) to jump far away, then continue. In GA, ensure that mating pairs are somewhat dissimilar (to produce varied offspring) – one can use diversity-based parent selection or occasionally introduce a brand new random individual into the population if diversity drops.
	•	Intensification vs. Diversification: Balance these two. Intensification means focus search around the best solutions found (to polish them), whereas diversification means explore new regions. Methods like Adaptive Memory or pool of elite solutions help with intensification (reuse good subroutes), while random perturbations and multi-start help diversification. You can schedule phases: e.g., run an intensification (local search or tabu from the current best) until no progress, then do a diversification kick (random restart or heavy perturbation), and repeat.
	•	Monitoring and Adjusting: Keep an eye on improvement over time. If you notice the algorithm stagnated early, that’s a sign to increase diversification (e.g., increase mutation in GA or do a bigger shake in ILS). If the algorithm is jumping around too much and not refining (e.g., SA still accepting many worse moves even late, or GA population too diverse and not converging), then intensify: lower SA temperature faster, or reduce mutation rate, etc. Ideally, the search should behave like a zoom lens: explore broadly at first, then gradually focus in on the promising region as time goes on ￼.
	•	Constraint Handling: One source of local optima could be the constraint that each family must have exactly m members. If your algorithm struggles with this (for instance, always oscillating between two family selections), consider handling that constraint in a smarter way. Maybe occasionally allow a temporary violation (serve m+1 and m-1 in two families) and then swap – this could reveal a path to a better configuration that was unreachable under strict adherence. This is an advanced idea and typically one would handle it by including a penalty for violating family count rather than a hard constraint during certain moves, then restoring feasibility. Only attempt if other methods fail to explore enough.
	•	Testing and Calibration: Use a set of test instances to calibrate your parameters and strategies. For example, you could take a smaller instance of F-CVRP and try different algorithms or parameter settings to see which gives the best result in 1 minute, then scale those settings to the larger instance. Open-source benchmark data (like F-CVRP instances if available, or analogous CVRP instances) can help validate the approach. Because the code must run within 5 minutes, pay attention to how the solution quality improves over time – you want most improvements to happen well before the cutoff. If, say, 90% of the improvement comes in the first 1 minute and little thereafter, you might stop earlier or try a different strategy for diminishing returns period (like switch from diversification to pure intensification for the last iterations).
	•	Final Tip – Modularity for Tuning: Because our Python code is modular, we can adjust parameters or swap out heuristics easily. Take advantage of that by trying, for instance, Tabu Search vs. SA vs. ILS simply by calling a different function, or toggling certain moves on/off to see their effect. This experimental flexibility is a strength of an open-source approach. Often a hybrid will work best: e.g., run SA for a while to escape a poor region, then switch to Tabu or ILS to intensify around a good solution, or use GA to generate a pool of candidates and then polish each with local search. By tuning parameters and combining strategies prudently, we can avoid poor local optima and push towards near-global optimum solutions.

In summary, solving the F-CVRP to high quality involves: choosing effective algorithms (and likely combining local search with a metaheuristic like ILS/Tabu/GA), implementing them in a modular way, and then carefully tuning their parameters and behavior. By following the above comparative insights, structured plan, and tuning tips, we leverage only open-source techniques to develop a solution approach that should yield excellent results within the 5-minute runtime on a modern PC. Each of the discussed algorithms brings something to the table – the best results will come from using their strengths in combination, ensuring robust search performance for this challenging family-constrained VRP.  ￼ ￼ (These references underscore that well-tuned heuristic approaches can efficiently find high-quality solutions for F-CVRP and related VRPs.)